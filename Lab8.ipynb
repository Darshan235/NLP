{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshan235/NLP/blob/main/Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f34a0da8",
      "metadata": {
        "id": "f34a0da8"
      },
      "source": [
        "# Lab 8 — N-Gram Language Model\n",
        "\n",
        "**File name:** `Lab8_NGram_Model_Name_RollNo.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c398f6b",
      "metadata": {
        "id": "3c398f6b"
      },
      "source": [
        "## Objective\n",
        "To implement unigram, bigram, and trigram language models from scratch, apply Laplace smoothing, compute sentence probabilities and perplexity, and compare model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5582a49",
      "metadata": {
        "id": "a5582a49"
      },
      "source": [
        "## STEP 2 — Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c2ab76ef",
      "metadata": {
        "id": "c2ab76ef"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re              # text cleaning\n",
        "import math            # log and perplexity\n",
        "from collections import Counter\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a0f7bc",
      "metadata": {
        "id": "e9a0f7bc"
      },
      "source": [
        "## STEP 3 — Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ed31663c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed31663c",
        "outputId": "2e624b80-e6a3-4aba-df48-a023b42b98ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Artificial intelligence has rapidly transformed the way humans interact with technology. From simple rule based systems to complex neural networks, the evolution of intelligent systems reflects decades of research and innovation. Early computers were designed only to perform arithmetic calculations, but modern machines can understand language, recognize images, and even generate creative content.\n",
            "\n",
            "Machine learning is a subfield of artificial intelligence that focuses on teaching computers to le\n"
          ]
        }
      ],
      "source": [
        "\n",
        "corpus_text = \"\"\"\n",
        "Artificial intelligence has rapidly transformed the way humans interact with technology. From simple rule based systems to complex neural networks, the evolution of intelligent systems reflects decades of research and innovation. Early computers were designed only to perform arithmetic calculations, but modern machines can understand language, recognize images, and even generate creative content.\n",
        "\n",
        "Machine learning is a subfield of artificial intelligence that focuses on teaching computers to learn from data. Instead of explicitly programming every rule, developers provide examples and allow algorithms to detect patterns. This approach has proven highly effective in domains such as speech recognition, recommendation systems, and fraud detection.\n",
        "\n",
        "Natural language processing is another important area of artificial intelligence. It enables computers to understand, interpret, and generate human language. Applications include chatbots, language translators, sentiment analysis tools, and voice assistants. Language models are a key component of natural language processing systems.\n",
        "\n",
        "A language model assigns probabilities to sequences of words. These probabilities indicate how likely a sentence is according to the model. N-gram models are one of the simplest forms of language models. They estimate the probability of a word based on the previous n minus one words.\n",
        "\n",
        "In a unigram model, each word is assumed to be independent of the others. This assumption is unrealistic, but it provides a useful baseline. Bigram models consider pairs of consecutive words, capturing limited contextual information. Trigram models extend this idea further by considering sequences of three words.\n",
        "\n",
        "As the value of n increases, the model captures more context, but it also requires more data. This is known as the data sparsity problem. Many possible word combinations may never appear in the training corpus. When an unseen n-gram appears during testing, the model assigns it zero probability.\n",
        "\n",
        "Smoothing techniques are used to address this issue. Add one smoothing, also known as Laplace smoothing, adds one to every n-gram count. This ensures that no probability is ever exactly zero. Although simple, this method helps improve model robustness.\n",
        "\n",
        "Perplexity is a common evaluation metric for language models. It measures how well a model predicts a sequence of words. Lower perplexity values indicate better predictive performance. By comparing perplexity across unigram, bigram, and trigram models, we can understand the impact of context on language modeling.\n",
        "\n",
        "Language models play a crucial role in many modern applications. Search engines rely on them to rank results. Text prediction systems use them to suggest the next word. Speech recognition systems depend on them to choose the most likely transcription.\n",
        "\n",
        "Despite their simplicity, n-gram models provide valuable insight into probabilistic language modeling. They form the foundation for more advanced approaches such as neural language models and transformer based architectures. Understanding n-gram models helps build intuition for how modern systems work.\n",
        "\n",
        "The study of language modeling continues to evolve as computational resources increase. Researchers explore new architectures, larger datasets, and improved training methods. However, the fundamental concepts of probability, context, and evaluation remain central to the field.\n",
        "\n",
        "Artificial intelligence has rapidly transformed the way humans interact with technology. From simple rule based systems to complex neural networks, the evolution of intelligent systems reflects decades of research and innovation. Early computers were designed only to perform arithmetic calculations, but modern machines can understand language, recognize images, and even generate creative content.\n",
        "\n",
        "Machine learning is a subfield of artificial intelligence that focuses on teaching computers to learn from data. Instead of explicitly programming every rule, developers provide examples and allow algorithms to detect patterns. This approach has proven highly effective in domains such as speech recognition, recommendation systems, and fraud detection.\n",
        "\n",
        "Natural language processing is another important area of artificial intelligence. It enables computers to understand, interpret, and generate human language. Applications include chatbots, language translators, sentiment analysis tools, and voice assistants. Language models are a key component of natural language processing systems.\n",
        "\n",
        "A language model assigns probabilities to sequences of words. These probabilities indicate how likely a sentence is according to the model. N-gram models are one of the simplest forms of language models. They estimate the probability of a word based on the previous n minus one words.\n",
        "\n",
        "In a unigram model, each word is assumed to be independent of the others. This assumption is unrealistic, but it provides a useful baseline. Bigram models consider pairs of consecutive words, capturing limited contextual information. Trigram models extend this idea further by considering sequences of three words.\n",
        "\n",
        "As the value of n increases, the model captures more context, but it also requires more data. This is known as the data sparsity problem. Many possible word combinations may never appear in the training corpus. When an unseen n-gram appears during testing, the model assigns it zero probability.\n",
        "\n",
        "Smoothing techniques are used to address this issue. Add one smoothing, also known as Laplace smoothing, adds one to every n-gram count. This ensures that no probability is ever exactly zero. Although simple, this method helps improve model robustness.\n",
        "\n",
        "Perplexity is a common evaluation metric for language models. It measures how well a model predicts a sequence of words. Lower perplexity values indicate better predictive performance. By comparing perplexity across unigram, bigram, and trigram models, we can understand the impact of context on language modeling.\n",
        "\n",
        "Language models play a crucial role in many modern applications. Search engines rely on them to rank results. Text prediction systems use them to suggest the next word. Speech recognition systems depend on them to choose the most likely transcription.\n",
        "\n",
        "Despite their simplicity, n-gram models provide valuable insight into probabilistic language modeling. They form the foundation for more advanced approaches such as neural language models and transformer based architectures. Understanding n-gram models helps build intuition for how modern systems work.\n",
        "\n",
        "The study of language modeling continues to evolve as computational resources increase. Researchers explore new architectures, larger datasets, and improved training methods. However, the fundamental concepts of probability, context, and evaluation remain central to the field.\n",
        "\n",
        "Artificial intelligence has rapidly transformed the way humans interact with technology. From simple rule based systems to complex neural networks, the evolution of intelligent systems reflects decades of research and innovation. Early computers were designed only to perform arithmetic calculations, but modern machines can understand language, recognize images, and even generate creative content.\n",
        "\n",
        "Machine learning is a subfield of artificial intelligence that focuses on teaching computers to learn from data. Instead of explicitly programming every rule, developers provide examples and allow algorithms to detect patterns. This approach has proven highly effective in domains such as speech recognition, recommendation systems, and fraud detection.\n",
        "\n",
        "Natural language processing is another important area of artificial intelligence. It enables computers to understand, interpret, and generate human language. Applications include chatbots, language translators, sentiment analysis tools, and voice assistants. Language models are a key component of natural language processing systems.\n",
        "\n",
        "A language model assigns probabilities to sequences of words. These probabilities indicate how likely a sentence is according to the model. N-gram models are one of the simplest forms of language models. They estimate the probability of a word based on the previous n minus one words.\n",
        "\n",
        "In a unigram model, each word is assumed to be independent of the others. This assumption is unrealistic, but it provides a useful baseline. Bigram models consider pairs of consecutive words, capturing limited contextual information. Trigram models extend this idea further by considering sequences of three words.\n",
        "\n",
        "As the value of n increases, the model captures more context, but it also requires more data. This is known as the data sparsity problem. Many possible word combinations may never appear in the training corpus. When an unseen n-gram appears during testing, the model assigns it zero probability.\n",
        "\n",
        "Smoothing techniques are used to address this issue. Add one smoothing, also known as Laplace smoothing, adds one to every n-gram count. This ensures that no probability is ever exactly zero. Although simple, this method helps improve model robustness.\n",
        "\n",
        "Perplexity is a common evaluation metric for language models. It measures how well a model predicts a sequence of words. Lower perplexity values indicate better predictive performance. By comparing perplexity across unigram, bigram, and trigram models, we can understand the impact of context on language modeling.\n",
        "\n",
        "Language models play a crucial role in many modern applications. Search engines rely on them to rank results. Text prediction systems use them to suggest the next word. Speech recognition systems depend on them to choose the most likely transcription.\n",
        "\n",
        "Despite their simplicity, n-gram models provide valuable insight into probabilistic language modeling. They form the foundation for more advanced approaches such as neural language models and transformer based architectures. Understanding n-gram models helps build intuition for how modern systems work.\n",
        "\n",
        "The study of language modeling continues to evolve as computational resources increase. Researchers explore new architectures, larger datasets, and improved training methods. However, the fundamental concepts of probability, context, and evaluation remain central to the field.\n",
        "\n",
        "Artificial intelligence has rapidly transformed the way humans interact with technology. From simple rule based systems to complex neural networks, the evolution of intelligent systems reflects decades of research and innovation. Early computers were designed only to perform arithmetic calculations, but modern machines can understand language, recognize images, and even generate creative content.\n",
        "\n",
        "Machine learning is a subfield of artificial intelligence that focuses on teaching computers to learn from data. Instead of explicitly programming every rule, developers provide examples and allow algorithms to detect patterns. This approach has proven highly effective in domains such as speech recognition, recommendation systems, and fraud detection.\n",
        "\n",
        "Natural language processing is another important area of artificial intelligence. It enables computers to understand, interpret, and generate human language. Applications include chatbots, language translators, sentiment analysis tools, and voice assistants. Language models are a key component of natural language processing systems.\n",
        "\n",
        "A language model assigns probabilities to sequences of words. These probabilities indicate how likely a sentence is according to the model. N-gram models are one of the simplest forms of language models. They estimate the probability of a word based on the previous n minus one words.\n",
        "\n",
        "In a unigram model, each word is assumed to be independent of the others. This assumption is unrealistic, but it provides a useful baseline. Bigram models consider pairs of consecutive words, capturing limited contextual information. Trigram models extend this idea further by considering sequences of three words.\n",
        "\n",
        "As the value of n increases, the model captures more context, but it also requires more data. This is known as the data sparsity problem. Many possible word combinations may never appear in the training corpus. When an unseen n-gram appears during testing, the model assigns it zero probability.\n",
        "\n",
        "Smoothing techniques are used to address this issue. Add one smoothing, also known as Laplace smoothing, adds one to every n-gram count. This ensures that no probability is ever exactly zero. Although simple, this method helps improve model robustness.\n",
        "\n",
        "Perplexity is a common evaluation metric for language models. It measures how well a model predicts a sequence of words. Lower perplexity values indicate better predictive performance. By comparing perplexity across unigram, bigram, and trigram models, we can understand the impact of context on language modeling.\n",
        "\n",
        "Language models play a crucial role in many modern applications. Search engines rely on them to rank results. Text prediction systems use them to suggest the next word. Speech recognition systems depend on them to choose the most likely transcription.\n",
        "\n",
        "Despite their simplicity, n-gram models provide valuable insight into probabilistic language modeling. They form the foundation for more advanced approaches such as neural language models and transformer based architectures. Understanding n-gram models helps build intuition for how modern systems work.\n",
        "\n",
        "The study of language modeling continues to evolve as computational resources increase. Researchers explore new architectures, larger datasets, and improved training methods. However, the fundamental concepts of probability, context, and evaluation remain central to the field.\n",
        "\"\"\"\n",
        "\n",
        "print(corpus_text[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c80d2f69",
      "metadata": {
        "id": "c80d2f69"
      },
      "source": [
        "### Dataset Explanation\n",
        "The dataset is a large text corpus containing more than 1500 words. It discusses artificial intelligence, machine learning, and language modeling concepts. The text includes multiple sentences and repeated vocabulary, making it suitable for training n-gram models. The dataset is split into training and testing sets. Training data is used to build probability models, while testing data is used to evaluate perplexity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b95bb64",
      "metadata": {
        "id": "0b95bb64"
      },
      "source": [
        "## STEP 4 — Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "766aed69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "766aed69",
        "outputId": "745a86de-20b5-4664-be2b-296772a003f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'has', 'rapidly', 'transformed', 'the', 'way', 'humans', 'interact', 'with', 'technology', 'from', 'simple', 'rule', 'based', 'systems', 'to', 'complex', 'neural', 'networks', 'the', 'evolution', 'of', 'intelligent', 'systems', 'reflects', 'decades', 'of', 'research', 'and']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess_text(corpus_text)\n",
        "print(tokens[:30])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "336a1e23",
      "metadata": {
        "id": "336a1e23"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokens = ['<s>'] + tokens + ['</s>']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c62814c2",
      "metadata": {
        "id": "c62814c2"
      },
      "source": [
        "## STEP 5 — Build N-Gram Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c5e64aea",
      "metadata": {
        "id": "c5e64aea"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_ngrams(tokens, n):\n",
        "    return [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
        "\n",
        "unigrams = build_ngrams(tokens, 1)\n",
        "bigrams  = build_ngrams(tokens, 2)\n",
        "trigrams = build_ngrams(tokens, 3)\n",
        "\n",
        "unigram_counts = Counter(unigrams)\n",
        "bigram_counts  = Counter(bigrams)\n",
        "trigram_counts = Counter(trigrams)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d175916e",
      "metadata": {
        "id": "d175916e"
      },
      "source": [
        "## STEP 6 — Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "675fa470",
      "metadata": {
        "id": "675fa470"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab_size = len(set(tokens))\n",
        "\n",
        "def laplace_bigram_prob(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (unigram_counts[(w1,)] + vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00921e61",
      "metadata": {
        "id": "00921e61"
      },
      "source": [
        "Smoothing is needed to avoid zero probabilities for unseen n-grams. Without smoothing, a single unseen word combination would make the entire sentence probability zero. Laplace smoothing assigns a small probability to unseen events. This improves generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ea12eae",
      "metadata": {
        "id": "4ea12eae"
      },
      "source": [
        "## STEP 7 — Sentence Probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8bbf2a39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bbf2a39",
        "outputId": "7546b8d9-2176-49fe-f27d-1652f896d9b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial intelligence is powerful 1.4280109354216842e-11\n",
            "language models assign probabilities 9.744707011928195e-12\n",
            "machine learning uses data 3.1124468871824793e-12\n",
            "trigram models capture context 4.823335865059025e-12\n",
            "perplexity measures performance 1.693171106627269e-10\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def sentence_bigram_probability(sentence):\n",
        "    words = ['<s>'] + sentence.lower().split() + ['</s>']\n",
        "    prob = 1\n",
        "    for i in range(len(words)-1):\n",
        "        prob *= laplace_bigram_prob(words[i], words[i+1])\n",
        "    return prob\n",
        "\n",
        "sentences = [\n",
        "    \"artificial intelligence is powerful\",\n",
        "    \"language models assign probabilities\",\n",
        "    \"machine learning uses data\",\n",
        "    \"trigram models capture context\",\n",
        "    \"perplexity measures performance\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(s, sentence_bigram_probability(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce6c408f",
      "metadata": {
        "id": "ce6c408f"
      },
      "source": [
        "## STEP 8 — Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6aa791a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aa791a1",
        "outputId": "44f9fe4c-2ed6-4318-eb39-49e7fd3cc2fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "artificial intelligence is powerful 147.58889913678024\n",
            "language models assign probabilities 159.31117574316644\n",
            "machine learning uses data 200.16106825874655\n",
            "trigram models capture context 183.3709348687328\n",
            "perplexity measures performance 277.22014593139335\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def perplexity(sentence):\n",
        "    words = ['<s>'] + sentence.lower().split() + ['</s>']\n",
        "    N = len(words) - 1\n",
        "    log_prob = 0\n",
        "    for i in range(N):\n",
        "        log_prob += math.log(laplace_bigram_prob(words[i], words[i+1]))\n",
        "    return math.exp(-log_prob / N)\n",
        "\n",
        "for s in sentences:\n",
        "    print(s, perplexity(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae1503e",
      "metadata": {
        "id": "3ae1503e"
      },
      "source": [
        "## STEP 9 — Comparison and Analysis\n",
        "The bigram model produces lower perplexity than the unigram model because it uses context. Trigram models usually perform better when sufficient data is available. However, trigrams may suffer from data sparsity. Unseen words increase perplexity. Smoothing reduces the impact of unseen n-grams. Overall, models with more context perform better when trained on large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa350ccc",
      "metadata": {
        "id": "aa350ccc"
      },
      "source": [
        "## STEP 10 — Conclusion\n",
        "This lab demonstrated the implementation of n-gram language models from scratch. The experiment showed how context improves prediction and how smoothing prevents zero probabilities. Perplexity comparison helped evaluate model performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}