{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshan235/NLP/blob/main/NLP_assignment_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0e6d29",
      "metadata": {
        "id": "2b0e6d29"
      },
      "source": [
        "\n",
        "# Lab12.4: Text Classification using 1D CNN with Pretrained GloVe Embeddings  \n",
        "\n",
        "Dataset: SMS Spam Collection  \n",
        "Pretrained Embeddings: GloVe 6B (100d)  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da639c3",
      "metadata": {
        "id": "9da639c3"
      },
      "source": [
        "## STEP 1 — Install & Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dc5dc83e",
      "metadata": {
        "id": "dc5dc83e"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install torch --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34046fc1",
      "metadata": {
        "id": "34046fc1"
      },
      "source": [
        "## STEP 2 — Download GloVe Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "967f4efc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "967f4efc",
        "outputId": "307a6ab6-072c-4e91-fd6c-0f6f68fdbacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-19 04:18:18--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2026-02-19 04:18:19--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2026-02-19 04:18:19--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.99MB/s    in 2m 41s  \n",
            "\n",
            "2026-02-19 04:21:00 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e70b6958",
      "metadata": {
        "id": "e70b6958"
      },
      "source": [
        "## STEP 3 — Load SMS Spam Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fcc60c2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcc60c2a",
        "outputId": "176f2127-e9c5-43d8-b40d-67623ea5de34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-19 04:21:58--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip’\n",
            "\n",
            "smsspamcollection.z     [ <=>                ] 198.65K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2026-02-19 04:21:59 (1.31 MB/s) - ‘smsspamcollection.zip’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n",
            "Dataset Size: 5572\n",
            "  label                                               text\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "!unzip -n smsspamcollection.zip\n",
        "!mv SMSSpamCollection SMSSpamCollection.tsv\n",
        "\n",
        "data = pd.read_csv('SMSSpamCollection.tsv', sep='\t', header=None, names=['label','text'])\n",
        "\n",
        "print(\"Dataset Size:\", len(data))\n",
        "print(data.head())\n",
        "\n",
        "data['label'] = data['label'].map({'ham':0, 'spam':1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adff185b",
      "metadata": {
        "id": "adff185b"
      },
      "source": [
        "## STEP 4 — Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "df658fd5",
      "metadata": {
        "id": "df658fd5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "data['text'] = data['text'].apply(clean_text)\n",
        "tokenized = data['text'].apply(lambda x: x.split())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c1752d",
      "metadata": {
        "id": "71c1752d"
      },
      "source": [
        "## STEP 5 — Vocabulary & Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ce84daca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce84daca",
        "outputId": "5d3207ea-bc28-4aee-fcaf-cbf6c15b620e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Matrix Shape: (8630, 100)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "vocab = {}\n",
        "for tokens in tokenized:\n",
        "    for word in tokens:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab) + 1\n",
        "\n",
        "vocab_size = len(vocab) + 1\n",
        "embedding_dim = 100\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = vector\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in vocab.items():\n",
        "    vector = embeddings_index.get(word)\n",
        "    if vector is not None:\n",
        "        embedding_matrix[idx] = vector\n",
        "\n",
        "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a46647a1",
      "metadata": {
        "id": "a46647a1"
      },
      "source": [
        "## STEP 6 — Padding & Train-Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "87ba5c0c",
      "metadata": {
        "id": "87ba5c0c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "max_len = 50\n",
        "\n",
        "def encode(tokens):\n",
        "    seq = [vocab.get(word, 0) for word in tokens]\n",
        "    if len(seq) < max_len:\n",
        "        seq += [0] * (max_len - len(seq))\n",
        "    else:\n",
        "        seq = seq[:max_len]\n",
        "    return seq\n",
        "\n",
        "X = np.array([encode(tokens) for tokens in tokenized])\n",
        "y = data['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SpamDataset(X_train, y_train)\n",
        "test_dataset = SpamDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e2742b",
      "metadata": {
        "id": "74e2742b"
      },
      "source": [
        "## STEP 7 — Define 1D CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "96f60882",
      "metadata": {
        "id": "96f60882"
      },
      "outputs": [],
      "source": [
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, embedding_matrix):\n",
        "        super(TextCNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight = nn.Parameter(\n",
        "            torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "        )\n",
        "        self.embedding.weight.requires_grad = False  # freeze embeddings\n",
        "\n",
        "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3)\n",
        "        self.conv2 = nn.Conv1d(embedding_dim, 128, kernel_size=4)\n",
        "        self.conv3 = nn.Conv1d(embedding_dim, 128, kernel_size=5)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.fc = nn.Linear(128 * 3, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        c1 = torch.relu(self.conv1(x))\n",
        "        c2 = torch.relu(self.conv2(x))\n",
        "        c3 = torch.relu(self.conv3(x))\n",
        "\n",
        "        p1 = torch.max(c1, dim=2)[0]\n",
        "        p2 = torch.max(c2, dim=2)[0]\n",
        "        p3 = torch.max(c3, dim=2)[0]\n",
        "\n",
        "        out = torch.cat([p1, p2, p3], dim=1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return torch.sigmoid(out)\n",
        "\n",
        "model = TextCNN(vocab_size, embedding_dim, embedding_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bddc58",
      "metadata": {
        "id": "70bddc58"
      },
      "source": [
        "## STEP 8 — Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "385f296a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "385f296a",
        "outputId": "6d4208ef-b0f2-4086-e2cc-73a9dbf5e949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.2434\n",
            "Epoch 2, Loss: 0.1021\n",
            "Epoch 3, Loss: 0.0667\n",
            "Epoch 4, Loss: 0.0477\n",
            "Epoch 5, Loss: 0.0323\n",
            "Epoch 6, Loss: 0.0270\n",
            "Epoch 7, Loss: 0.0202\n",
            "Epoch 8, Loss: 0.0151\n",
            "Epoch 9, Loss: 0.0119\n",
            "Epoch 10, Loss: 0.0096\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04bb34ab",
      "metadata": {
        "id": "04bb34ab"
      },
      "source": [
        "## STEP 9 — Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXT_L03KKKdl"
      },
      "id": "WXT_L03KKKdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b2533f6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2533f6d",
        "outputId": "d29bab63-6cdb-40c8-d5ff-5628a9d15027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.979372197309417\n",
            "Precision: 0.9565217391304348\n",
            "Recall: 0.8859060402684564\n",
            "F1-score: 0.9198606271777003\n",
            "\n",
            "Confusion Matrix:\n",
            " [[960   6]\n",
            " [ 17 132]]\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        outputs = model(X_batch).squeeze()\n",
        "        preds = (outputs > 0.5).int().cpu().numpy()\n",
        "\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(y_batch.numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision = precision_score(all_labels, all_preds)\n",
        "recall = recall_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nConfusion Matrix:\\n\", cm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f43ac1",
      "metadata": {
        "id": "b7f43ac1"
      },
      "source": [
        "\n",
        "## STEP 10 — Result Analysis\n",
        "\n",
        "Pretrained GloVe embeddings provide semantic information.\n",
        "CNN captures local n-gram features.\n",
        "Embeddings improve convergence and classification quality.\n",
        "Spam detection benefits from distinctive word patterns.\n",
        "Limitations include simple CNN architecture and fixed embeddings.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}