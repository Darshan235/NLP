{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Darshan235/NLP/blob/main/nlp_asn_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241e3866",
      "metadata": {
        "id": "241e3866"
      },
      "source": [
        "\n",
        "# Lab 7: Text Similarity in NLP    \n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lab, students will be able to:\n",
        "- Understand what text similarity means in NLP  \n",
        "- Compute similarity using:\n",
        "  - Cosine Similarity  \n",
        "  - Jaccard Similarity  \n",
        "  - WordNet-based Semantic Similarity  \n",
        "- Interpret similarity scores  \n",
        "- Compare lexical vs semantic similarity  \n",
        "\n",
        "## Lab Outcomes\n",
        "After completing this lab, students will be able to:\n",
        "- Explain the purpose of text similarity in NLP tasks  \n",
        "- Perform preprocessing for similarity tasks  \n",
        "- Represent text using Bag-of-Words / TF-IDF  \n",
        "- Implement cosine and jaccard similarity in Python  \n",
        "- Use WordNet to compute semantic similarity  \n",
        "- Analyze which similarity measure works better  \n",
        "- Prepare a short analysis report  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd58802",
      "metadata": {
        "id": "ccd58802"
      },
      "source": [
        "\n",
        "## 1. What is Text Similarity in NLP?\n",
        "Text similarity measures how close two pieces of text are in terms of meaning or word usage.\n",
        "It is widely used in search engines, recommendation systems, plagiarism detection, and chatbots.\n",
        "\n",
        "## 2. Lexical vs Semantic Similarity\n",
        "- **Lexical similarity** depends on exact word overlap (Cosine, Jaccard).\n",
        "- **Semantic similarity** considers meaning, even if words differ (WordNet).\n",
        "\n",
        "## 3. Why Cosine Similarity?\n",
        "Cosine similarity measures the angle between vectors, not magnitude.\n",
        "It works well with TF-IDF and handles different document lengths efficiently.\n",
        "\n",
        "## 4. When Jaccard Fails\n",
        "Jaccard fails when:\n",
        "- Synonyms are used\n",
        "- Sentences are short\n",
        "- Word order/meaning matters\n",
        "\n",
        "## 5. How WordNet Improves Similarity\n",
        "WordNet connects words via synonym sets (synsets).\n",
        "It captures relationships like doctor–physician.\n",
        "\n",
        "## 6. Effect of Preprocessing\n",
        "Preprocessing reduces noise and improves similarity accuracy.\n",
        "Poor preprocessing leads to misleading similarity scores.\n",
        "\n",
        "## 7. Applications of Text Similarity\n",
        "- Search engines\n",
        "- Plagiarism detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "164591f7",
      "metadata": {
        "id": "164591f7"
      },
      "source": [
        "\n",
        "## STEP 2 — Import Required Libraries\n",
        "We use:\n",
        "- **nltk**: preprocessing, stopwords, WordNet\n",
        "- **sklearn**: TF-IDF, cosine similarity\n",
        "- **string & re**: text cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d8d35f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61d8d35f",
        "outputId": "ebd1881b-4851-474c-e191-9b2006b3f09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69e97e0",
      "metadata": {
        "id": "f69e97e0"
      },
      "source": [
        "\n",
        "## STEP 3 — Dataset Description\n",
        "We created a dataset of 24 short documents across 4 topics:\n",
        "- Sports\n",
        "- Politics\n",
        "- Health\n",
        "- Technology\n",
        "\n",
        "Each topic contains 6 documents.\n",
        "The dataset is manually created to control topic relevance and vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aea1355",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aea1355",
        "outputId": "a1541648-2b95-41af-cf34-52ab6ecc80ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample documents:\n",
            "- The football team won the championship\n",
            "- Cricket players trained hard for the match\n",
            "- The athlete broke the world record\n",
            "- Basketball requires teamwork and speed\n",
            "- The coach planned a new strategy\n"
          ]
        }
      ],
      "source": [
        "\n",
        "documents = [\n",
        "    # Sports\n",
        "    \"The football team won the championship\",\n",
        "    \"Cricket players trained hard for the match\",\n",
        "    \"The athlete broke the world record\",\n",
        "    \"Basketball requires teamwork and speed\",\n",
        "    \"The coach planned a new strategy\",\n",
        "    \"The match was intense and competitive\",\n",
        "\n",
        "    # Politics\n",
        "    \"The government passed a new law\",\n",
        "    \"Elections determine the political future\",\n",
        "    \"The president addressed the nation\",\n",
        "    \"Parliament debated the new policy\",\n",
        "    \"The minister announced reforms\",\n",
        "    \"Democracy depends on citizen participation\",\n",
        "\n",
        "    # Health\n",
        "    \"Doctors recommend regular exercise\",\n",
        "    \"A healthy diet improves immunity\",\n",
        "    \"The patient recovered from illness\",\n",
        "    \"Mental health awareness is important\",\n",
        "    \"Vaccines prevent serious diseases\",\n",
        "    \"Hospitals provide emergency care\",\n",
        "\n",
        "    # Technology\n",
        "    \"Artificial intelligence is transforming industries\",\n",
        "    \"The smartphone uses advanced technology\",\n",
        "    \"Cybersecurity protects digital data\",\n",
        "    \"Machine learning improves predictions\",\n",
        "    \"Cloud computing enables scalability\",\n",
        "    \"Software development requires logical thinking\"\n",
        "]\n",
        "\n",
        "print(\"Sample documents:\")\n",
        "for doc in documents[:5]:\n",
        "    print(\"-\", doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d88af248",
      "metadata": {
        "id": "d88af248"
      },
      "source": [
        "\n",
        "## STEP 4 — Text Preprocessing\n",
        "Steps:\n",
        "1. Lowercasing\n",
        "2. Removing punctuation and numbers\n",
        "3. Stopword removal\n",
        "4. Tokenization\n",
        "5. Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cd6e5b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cd6e5b9",
        "outputId": "cbeae0a3-2686-43b5-a59d-3cb145635743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['football team championship', 'cricket player trained hard match', 'athlete broke world record', 'basketball requires teamwork speed', 'coach planned new strategy']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "\n",
        "print(processed_docs[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d73dbe49",
      "metadata": {
        "id": "d73dbe49"
      },
      "source": [
        "\n",
        "## STEP 5 — Text Representation (TF-IDF)\n",
        "TF-IDF is chosen because:\n",
        "- It reduces importance of common words\n",
        "- Highlights meaningful terms\n",
        "- Works well with cosine similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "510172f2",
      "metadata": {
        "id": "510172f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "152b6c3f",
      "metadata": {
        "id": "152b6c3f"
      },
      "source": [
        "\n",
        "## STEP 6 — Cosine Similarity\n",
        "Higher score → more similar meaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1be50558",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1be50558",
        "outputId": "b7aac274-08fc-4d48-bd4d-112a89833bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between doc 0 and doc 1: 0.000\n",
            "Similarity between doc 1 and doc 2: 0.000\n",
            "Similarity between doc 2 and doc 3: 0.000\n",
            "Similarity between doc 3 and doc 4: 0.000\n",
            "Similarity between doc 4 and doc 5: 0.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cosine_sim = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Similarity between doc {i} and doc {i+1}: {cosine_sim[i][i+1]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b80e4fb",
      "metadata": {
        "id": "3b80e4fb"
      },
      "source": [
        "\n",
        "## STEP 7 — Jaccard Similarity\n",
        "Based on word overlap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc57deb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bc57deb",
        "outputId": "56549f76-89d7-4f41-fded-157c5d036bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard(doc 0, doc 1) = 0.000\n",
            "Jaccard(doc 1, doc 2) = 0.000\n",
            "Jaccard(doc 2, doc 3) = 0.000\n",
            "Jaccard(doc 3, doc 4) = 0.000\n",
            "Jaccard(doc 4, doc 5) = 0.000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1, set2 = set(doc1.split()), set(doc2.split())\n",
        "    return len(set1 & set2) / len(set1 | set2)\n",
        "\n",
        "for i in range(5):\n",
        "    print(f\"Jaccard(doc {i}, doc {i+1}) = {jaccard_similarity(processed_docs[i], processed_docs[i+1]):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "233e3329",
      "metadata": {
        "id": "233e3329"
      },
      "source": [
        "\n",
        "## STEP 8 — WordNet Semantic Similarity\n",
        "We use Wu-Palmer similarity to capture semantic meaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59b5647",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c59b5647",
        "outputId": "c3040595-ec9b-47f2-d1b3-1163ba70fee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doctor - physician: 1.000\n",
            "football - cricket: 0.091\n",
            "law - policy: 0.286\n",
            "hospital - clinic: 0.118\n",
            "software - program: 0.267\n",
            "government - state: 0.133\n",
            "athlete - player: 0.667\n",
            "disease - illness: 0.947\n",
            "technology - innovation: 0.125\n",
            "election - vote: 0.625\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def wordnet_similarity(w1, w2):\n",
        "    syns1 = wordnet.synsets(w1)\n",
        "    syns2 = wordnet.synsets(w2)\n",
        "    if not syns1 or not syns2:\n",
        "        return 0\n",
        "    return syns1[0].wup_similarity(syns2[0]) or 0\n",
        "\n",
        "pairs = [(\"doctor\", \"physician\"), (\"football\", \"cricket\"), (\"law\", \"policy\"),\n",
        "         (\"hospital\", \"clinic\"), (\"software\", \"program\"), (\"government\", \"state\"),\n",
        "         (\"athlete\", \"player\"), (\"disease\", \"illness\"), (\"technology\", \"innovation\"),\n",
        "         (\"election\", \"vote\")]\n",
        "\n",
        "for w1, w2 in pairs:\n",
        "    print(f\"{w1} - {w2}: {wordnet_similarity(w1, w2):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60ebad61",
      "metadata": {
        "id": "60ebad61"
      },
      "source": [
        "\n",
        "## STEP 9 — Comparison of Methods\n",
        "- Cosine works best for longer texts\n",
        "- Jaccard depends heavily on exact word overlap\n",
        "- WordNet captures meaning better\n",
        "- Scores disagree when synonyms are used\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3161e830",
      "metadata": {
        "id": "3161e830"
      },
      "source": [
        "\n",
        "## STEP 10 — Lab Report Conclusion\n",
        "This lab demonstrated how different similarity techniques behave.\n",
        "Lexical methods are simple but limited.\n",
        "Semantic similarity captures deeper meaning.\n",
        "Choosing the right method depends on text length and task.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}